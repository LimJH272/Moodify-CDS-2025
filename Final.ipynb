{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbsENskWPe9k",
        "outputId": "6193bbc4-4f20-4c16-ed26-31f798e9c389"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdkCbRSGPkQ3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from collections import Counter\n",
        "import sys\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSMBm9RHP2N5"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "TARGET_SAMPLE_RATE = 16000\n",
        "TARGET_AUDIO_LENGTH = 10 * TARGET_SAMPLE_RATE\n",
        "LABEL_TO_INT = {\"Happy\": 0, \"Sad\": 1, \"Fear\": 2, \"Anger\": 3}\n",
        "INT_TO_LABEL = {0: \"Happy\", 1: \"Sad\", 2: \"Fear\", 3: \"Anger\"}\n",
        "\n",
        "desired_emotions = list(LABEL_TO_INT.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32-U-4QlQHgj"
      },
      "outputs": [],
      "source": [
        "def load_dataset(csv_path, audio_dir):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df[\"Emotion\"] = df[\"Emotion\"].str.capitalize()\n",
        "    df = df[df[\"Emotion\"].isin(desired_emotions)].reset_index(drop=True)\n",
        "    df[\"File\"] = df[\"Nro\"].astype(str) + \".mp3\"\n",
        "    df[\"Aud_dir\"] = audio_dir\n",
        "    return df\n",
        "\n",
        "def load_dataset2(csv_path, audio_dir):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    df[\"Emotion\"] = df[\"Emotion\"].str.capitalize()\n",
        "    df = df[df[\"Emotion\"].isin(desired_emotions)].reset_index(drop=True)\n",
        "    df[\"File\"] = df[\"Nro\"].astype(str).str.zfill(3) + \".mp3\"\n",
        "    df[\"Aud_dir\"] = audio_dir\n",
        "    print(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3V_29TZwQKkl",
        "outputId": "690f71c1-7cf0-4824-f22b-136f65549cb3"
      },
      "outputs": [],
      "source": [
        "df1 = load_dataset2(\"/content/drive/MyDrive/Moodify-CDS-2025/data/set1_tracklist.csv\", \"/content/drive/MyDrive/Moodify-CDS-2025/data/Set1\")\n",
        "df2 = load_dataset(\"/content/drive/MyDrive/Moodify-CDS-2025/aggregated_emotions_for_model_1.csv\", \"/content/drive/MyDrive/Moodify-CDS-2025/emotifymusic/pop\")\n",
        "df_combined_balanced = pd.concat([df1, df2]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISBvd_RuU-Hg"
      },
      "outputs": [],
      "source": [
        "class Stereo2Mono(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.weights = torch.tensor([0.5, 0.5], requires_grad=False).view(2, 1)\n",
        "\n",
        "    def forward(self, waveform: torch.Tensor) -> torch.Tensor:\n",
        "        if waveform.shape[0] == 2:\n",
        "            return torch.sum(waveform * self.weights, dim=0) * np.sqrt(2)\n",
        "        return waveform\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWn-3PMEV23q"
      },
      "outputs": [],
      "source": [
        "class AudioEmotionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.dataframe = dataframe\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        self.prepare_dataset()\n",
        "\n",
        "    def prepare_dataset(self):\n",
        "      self.dataframe.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "      for _, row in tqdm(self.dataframe.iterrows(), total=len(self.dataframe)):\n",
        "          audio_path = os.path.join(row[\"Aud_dir\"], row[\"File\"])\n",
        "            # label = row[\"Emotion\"]\n",
        "          if label not in LABEL_TO_INT:\n",
        "              print(f\"Skipping invalid label: {label}\")\n",
        "              continue\n",
        "          label_int = LABEL_TO_INT[label]\n",
        "          # print(label_int)\n",
        "          try:\n",
        "              wf = path_to_waveform_tensor(audio_path)\n",
        "              segments, segment_labels = segmentate_waveforms([wf], [label_int], train=True)\n",
        "              # print(segments, segment_labels)\n",
        "              self.data.extend(segments)\n",
        "              self.labels.extend(segment_labels)\n",
        "          except Exception as e:\n",
        "              print(f\"Error loading {audio_path}: {e}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByV1stAvV5J1"
      },
      "outputs": [],
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.melspec = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=TARGET_SAMPLE_RATE, n_mels=128, n_fft=4096, center=True\n",
        "        )\n",
        "        self.normalise = lambda x: torch.from_numpy(librosa.power_to_db(x.cpu().numpy(), ref=np.max))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.melspec(x)\n",
        "        x = self.normalise(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFv6b-4wDzpF"
      },
      "source": [
        "Refrences:\n",
        "\n",
        "\n",
        "Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2017). Attention is all you need. arXiv (Cornell University). https://doi.org/10.48550/arxiv.1706.03762\n",
        "\n",
        "Jeff Heaton. (2023, November 1). Transformer-Based Time Series with PyTorch (10.3) [Video]. YouTube. https://www.youtube.com/watch?v=NGzQpphf_Vc",
        "\n",
        "StatQuest with Josh Starmer. (2024, July 1). Coding a ChatGPT like transformer from scratch in PyTorch [Video]. YouTube. https://www.youtube.com/watch?v=C9QSpl5nmrY",

      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyzSrnflWL1Y"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.pow(10000.0, torch.arange(0, d_model, 2).float() / d_model)\n",
        "        pe[:, 0::2] = torch.sin(position /div_term)\n",
        "        pe[:, 1::2] = torch.cos(position/ div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAxCsSSuWoWa"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(dim, dim)\n",
        "        self.key = nn.Linear(dim, dim)\n",
        "        self.value = nn.Linear(dim, dim)\n",
        "        self.key_dim = dim\n",
        "        self.out_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_weights = torch.matmul(self.query(x), self.key(x).transpose(-2, -1))\n",
        "        attn_weights = attn_weights / self.key_dim ** 0.5\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attended = torch.matmul(attn_weights, self.value(x))\n",
        "        return self.out_proj(attended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSr02tyGWmZb"
      },
      "outputs": [],
      "source": [
        "class MusicTransformerJAZZ(nn.Module):\n",
        "    def __init__(self, input_dim=128, num_classes=4, nhead=8, num_layers=3, dropout=0.7):\n",
        "        super().__init__()\n",
        "        self.input = nn.Linear(input_dim, input_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, input_dim))\n",
        "        self.position_encoder = PositionalEncoding(input_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            input_dim, nhead=nhead, dim_feedforward=512, dropout=dropout, activation=\"gelu\"\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(input_dim)\n",
        "        \n",
        "\n",
        "        self.self_attention = SelfAttentionBlock(input_dim)\n",
        "        # self.dropout = nn.Dropout(dropout)*5  \n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        self.dropout4 = nn.Dropout(dropout)\n",
        "        self.dropout5 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.input(x)\n",
        "        batch_size, _, dim_emb = x.shape\n",
        "        cls_tokens = self.cls_token.expand(batch_size, 1, dim_emb)\n",
        "        x = torch.cat([cls_tokens, x], dim=1)\n",
        "\n",
        "        x = self.position_encoder(x)\n",
        "        x = self.norm(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.permute(1, 0, 2)\n",
        "        x = self.self_attention(x)\n",
        "        cls_rep = x[:, 0]\n",
        "\n",
        "        out1 = self.fc_out(self.dropout1(cls_rep  ))\n",
        "        out2 = self.fc_out(self.dropout2(cls_rep))\n",
        "        out3 = self.fc_out(self.dropout3(cls_rep))\n",
        "        out4 = self.fc_out(self.dropout4(cls_rep))\n",
        "        out5 = self.fc_out(self.dropout5(cls_rep))\n",
        "\n",
        "        out = torch.stack([out1, out2, out3, out4, out5], dim=0)\n",
        "\n",
        "        return out.mean(dim=0)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8uNAM9YVKCi"
      },
      "outputs": [],
      "source": [
        "def path_to_waveform_tensor(path: str, sample_rate=TARGET_SAMPLE_RATE):\n",
        "    waveform, sr = torchaudio.load(path)\n",
        "    if sr != sample_rate:\n",
        "        waveform = torchaudio.transforms.Resample(sr, sample_rate)(waveform)\n",
        "    mono_wf = Stereo2Mono()(waveform)\n",
        "    return mono_wf.squeeze(0).cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO_Wt1E9VMO7"
      },
      "outputs": [],
      "source": [
        "def split_waveform_segments(wf: torch.Tensor, target_len=TARGET_AUDIO_LENGTH, min_length: int=TARGET_SAMPLE_RATE * 10, train: bool=False) -> list[torch.Tensor]:\n",
        "    assert len(wf.shape) == 1\n",
        "    wf_len = wf.shape[0]\n",
        "\n",
        "    if wf_len < target_len:\n",
        "        return [wf.detach().clone()] if wf_len >= min_length else []\n",
        "    else:\n",
        "        hop_length = target_len // 2 if train else target_len\n",
        "        tensor_ls = []\n",
        "\n",
        "        for i in range(0, wf_len, hop_length):\n",
        "            if wf_len - i > target_len:\n",
        "                tensor_ls.append(wf[i:i+target_len])\n",
        "            else:\n",
        "                if wf_len - i >= min_length:\n",
        "                    tensor_ls.append(wf[(-target_len if train else i):])\n",
        "\n",
        "        return tensor_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIETdIeyVM4S"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "    waveforms, labels = zip(*batch)\n",
        "    waveforms = pad_sequence(waveforms, batch_first=True)\n",
        "    labels = torch.tensor(labels)\n",
        "    return waveforms, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arzjyHN8VNvf"
      },
      "outputs": [],
      "source": [
        "def segmentate_waveforms(wf_ls: list[torch.Tensor], label_ls: list[int], train: bool=False):\n",
        "    new_wf_ls, new_label_ls = [], []\n",
        "\n",
        "    for wf, label in zip(wf_ls, label_ls):\n",
        "        wf_segments = split_waveform_segments(wf, train=train)\n",
        "        new_wf_ls.extend(wf_segments)\n",
        "        new_label_ls.extend([label] * len(wf_segments))\n",
        "\n",
        "    return new_wf_ls, new_label_ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbW2EE1EPyI2",
        "outputId": "7fe45331-a81e-4cfb-9d0f-c9d9c1cc9eef"
      },
      "outputs": [],
      "source": [
        "full_dataset = AudioEmotionDataset(df_combined_balanced)\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBdFmBWXXkdD",
        "outputId": "821a023f-5234-4553-8587-191688496e7a"
      },
      "outputs": [],
      "source": [
        "extractor = FeatureExtractor().to(device)\n",
        "model = MusicTransformerJAZZ().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jRq_gKhjXhOk",
        "outputId": "47bd015d-8a3d-4a15-9be1-d8f4565e6544"
      },
      "outputs": [],
      "source": [
        "\n",
        "epochs = 160\n",
        "for epoch in range(epochs):\n",
        "    if epoch>=100:\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-7)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\"):\n",
        "        x, y = batch\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x_feat = torch.stack([extractor(wf) for wf in x]).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x_feat)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Training Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "    # --- Evaluation Phase ---\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} - Evaluating\"):\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            x_feat = torch.stack([extractor(wf) for wf in x]).to(device)\n",
        "            outputs = model(x_feat)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            y_true.extend(y.cpu().numpy())\n",
        "            y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "\n",
        "    print(f\"\\nEpoch {epoch+1} Predictions:\")\n",
        "    for true, pred in zip(y_true, y_pred):\n",
        "        print(f\"True: {INT_TO_LABEL[true]} | Pred: {INT_TO_LABEL[pred]}\")\n",
        "\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(\n",
        "    y_true,\n",
        "    y_pred,\n",
        "    labels=list(INT_TO_LABEL.keys()),\n",
        "    target_names=list(INT_TO_LABEL.values())\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrs4-3kaX6W7",
        "outputId": "3a934b44-9d9a-4e1e-d5b4-ac71361e62e8"
      },
      "outputs": [],
      "source": [
        "model_save_path = \"/content/drive/MyDrive/main_final_fin.pth\"\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved to {model_save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        },
        "id": "yNurxexAYeAx",
        "outputId": "3c008eea-1b97-4edc-e6ab-c103a3ceaca9"
      },
      "outputs": [],
      "source": [
        "model = MusicTransformerJAZZ().to(device)\n",
        "model_load_path = \"/content/drive/MyDrive/main_final_1.pth\"\n",
        "model.load_state_dict(torch.load(model_load_path,map_location=torch.device('cpu')))\n",
        "\n",
        "model.eval()\n",
        "y_true, y_pred = [], []\n",
        "with torch.no_grad():\n",
        "    for x, y in tqdm(val_loader, desc=\"Evaluating\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        x_feat = torch.stack([extractor(wf) for wf in x]).to(device)\n",
        "        outputs = model(x_feat)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "        y_true.extend(y.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(y_true, y_pred, target_names=INT_TO_LABEL.values()))\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=INT_TO_LABEL.values(), yticklabels=INT_TO_LABEL.values())\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4d2CY_KahLs",
        "outputId": "ce1389a8-4a7f-4507-b6e0-2d003cf919c6"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNgZRklgQdQu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
